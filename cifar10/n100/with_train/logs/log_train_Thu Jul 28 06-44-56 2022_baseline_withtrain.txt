2022-07-28 06:44:56,540: INFO: Hyperparameters
{'augment': False,
 'backbone': 'alexnet',
 'batch_size': 1000,
 'class_balanced': False,
 'config': 'src/configs/cifar10.yml',
 'criterion': 'NLLLoss',
 'criterion_kwargs': None,
 'dataset': 'cifar10',
 'dataset_dir': './data',
 'dont_train': False,
 'early_stopping_kwargs': {'min_delta': 0.001,
                           'min_epochs': 200,
                           'patience': 50},
 'epochs': 1000,
 'input_shape': [3, 32, 32],
 'logdir': PosixPath('cifar10/n100/with_train/logs'),
 'lr': 0.001,
 'num_classes': 10,
 'num_workers': 2,
 'optimizer': 'sgd',
 'optimizer_kwargs': {'momentum': 0.9, 'weight_decay': 0.01},
 'output_dir': PosixPath('cifar10/n100/with_train'),
 'per_class': False,
 'random': False,
 'resume': None,
 'scheduler': None,
 'seed': 0,
 'test_model': None,
 'topn': 100,
 'transformation_kwargs': {'normalize': {'mean': [0.4914, 0.4822, 0.4465],
                                         'std': [0.2023, 0.1994, 0.201]}},
 'use_saved_best_inds': None,
 'val_percent': 0.1,
 'wandb': False,
 'with_train': True}
2022-07-28 06:44:57,462: INFO: Dataset
Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
2022-07-28 06:44:58,195: INFO: Test Dataset
Dataset CIFAR10
    Number of datapoints: 10000
    Root location: ./data
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
2022-07-28 06:44:58,199: INFO: Loading similarities from cifar10/all_similarities_withtrain.npy
Loading imginds from cifar10/all_imginds_withtrain.npy
2022-07-28 06:44:58,218: INFO: all_similarities.shape: (100, 50000), all_imginds.shape: (100, 50000)
2022-07-28 06:45:02,588: INFO: Model Summary
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
├─Sequential: 1-1                        [-1, 1024]                --
|    └─Conv2d: 2-1                       [-1, 64, 16, 16]          1,792
|    └─MaxPool2d: 2-2                    [-1, 64, 8, 8]            --
|    └─ReLU: 2-3                         [-1, 64, 8, 8]            --
|    └─Conv2d: 2-4                       [-1, 192, 8, 8]           110,784
|    └─MaxPool2d: 2-5                    [-1, 192, 4, 4]           --
|    └─ReLU: 2-6                         [-1, 192, 4, 4]           --
|    └─Conv2d: 2-7                       [-1, 384, 4, 4]           663,936
|    └─ReLU: 2-8                         [-1, 384, 4, 4]           --
|    └─Conv2d: 2-9                       [-1, 256, 4, 4]           884,992
|    └─ReLU: 2-10                        [-1, 256, 4, 4]           --
|    └─Conv2d: 2-11                      [-1, 256, 4, 4]           590,080
|    └─MaxPool2d: 2-12                   [-1, 256, 2, 2]           --
|    └─ReLU: 2-13                        [-1, 256, 2, 2]           --
|    └─Flatten: 2-14                     [-1, 1024]                --
├─Sequential: 1-2                        [-1, 10]                  --
|    └─Dropout: 2-15                     [-1, 1024]                --
|    └─Linear: 2-16                      [-1, 512]                 524,800
|    └─ReLU: 2-17                        [-1, 512]                 --
|    └─Dropout: 2-18                     [-1, 512]                 --
|    └─Linear: 2-19                      [-1, 256]                 131,328
|    └─ReLU: 2-20                        [-1, 256]                 --
|    └─Linear: 2-21                      [-1, 10]                  2,570
|    └─LogSoftmax: 2-22                  [-1, 10]                  --
==========================================================================================
Total params: 2,910,282
Trainable params: 2,910,282
Non-trainable params: 0
Total mult-adds (M): 45.30
==========================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 0.33
Params size (MB): 11.10
Estimated Total Size (MB): 11.45
==========================================================================================
2022-07-28 06:45:05,316: INFO: Epoch[   1] Loss: 10.35	Accuracy: 8.889	Val_Loss: 3.319	Val_Acc: 40.000
2022-07-28 06:45:05,316: INFO: Epoch[   1] Test Accuracy: 10.230
2022-07-28 06:45:10,044: INFO: Epoch[  51] Loss: 1.91	Accuracy: 31.111	Val_Loss: 2.099	Val_Acc: 30.000
2022-07-28 06:45:10,044: INFO: Epoch[  51] Test Accuracy: 15.970
2022-07-28 06:45:14,992: INFO: Epoch[ 101] Loss: 1.36	Accuracy: 55.556	Val_Loss: 2.152	Val_Acc: 30.000
2022-07-28 06:45:14,992: INFO: Epoch[ 101] Test Accuracy: 17.370
2022-07-28 06:45:19,948: INFO: Epoch[ 151] Loss: 0.94	Accuracy: 72.222	Val_Loss: 2.154	Val_Acc: 40.000
2022-07-28 06:45:19,948: INFO: Epoch[ 151] Test Accuracy: 20.080
2022-07-28 06:45:24,885: INFO: Epoch[ 201] Loss: 0.45	Accuracy: 91.111	Val_Loss: 2.384	Val_Acc: 40.000
2022-07-28 06:45:24,885: INFO: Epoch[ 201] Test Accuracy: 20.870
2022-07-28 06:45:25,050: INFO: Epoch: 204 Early stopping counter 1 of 50
2022-07-28 06:45:25,114: INFO: Epoch: 205 Early stopping counter 2 of 50
2022-07-28 06:45:25,180: INFO: Epoch: 206 Early stopping counter 3 of 50
2022-07-28 06:45:25,241: INFO: Epoch: 207 Early stopping counter 4 of 50
2022-07-28 06:45:25,303: INFO: Epoch: 208 Early stopping counter 5 of 50
2022-07-28 06:45:25,364: INFO: Epoch: 209 Early stopping counter 6 of 50
2022-07-28 06:45:25,423: INFO: Epoch: 210 Early stopping counter 7 of 50
2022-07-28 06:45:25,484: INFO: Epoch: 211 Early stopping counter 8 of 50
2022-07-28 06:45:25,546: INFO: Epoch: 212 Early stopping counter 9 of 50
2022-07-28 06:45:25,609: INFO: Epoch: 213 Early stopping counter 10 of 50
2022-07-28 06:45:25,670: INFO: Epoch: 214 Early stopping counter 11 of 50
2022-07-28 06:45:25,730: INFO: Epoch: 215 Early stopping counter 12 of 50
2022-07-28 06:45:25,794: INFO: Epoch: 216 Early stopping counter 13 of 50
2022-07-28 06:45:25,864: INFO: Epoch: 217 Early stopping counter 14 of 50
2022-07-28 06:45:25,939: INFO: Epoch: 218 Early stopping counter 15 of 50
2022-07-28 06:45:26,004: INFO: Epoch: 219 Early stopping counter 16 of 50
2022-07-28 06:45:26,072: INFO: Epoch: 220 Early stopping counter 17 of 50
2022-07-28 06:45:26,136: INFO: Epoch: 221 Early stopping counter 18 of 50
2022-07-28 06:45:26,199: INFO: Epoch: 222 Early stopping counter 19 of 50
2022-07-28 06:45:26,267: INFO: Epoch: 223 Early stopping counter 20 of 50
2022-07-28 06:45:26,334: INFO: Epoch: 224 Early stopping counter 21 of 50
2022-07-28 06:45:26,397: INFO: Epoch: 225 Early stopping counter 22 of 50
2022-07-28 06:45:26,471: INFO: Epoch: 226 Early stopping counter 23 of 50
2022-07-28 06:45:26,541: INFO: Epoch: 227 Early stopping counter 24 of 50
2022-07-28 06:45:26,607: INFO: Epoch: 228 Early stopping counter 25 of 50
2022-07-28 06:45:26,671: INFO: Epoch: 229 Early stopping counter 26 of 50
2022-07-28 06:45:26,733: INFO: Epoch: 230 Early stopping counter 27 of 50
2022-07-28 06:45:26,794: INFO: Epoch: 231 Early stopping counter 28 of 50
2022-07-28 06:45:26,857: INFO: Epoch: 232 Early stopping counter 29 of 50
2022-07-28 06:45:26,928: INFO: Epoch: 233 Early stopping counter 30 of 50
2022-07-28 06:45:26,998: INFO: Epoch: 234 Early stopping counter 31 of 50
2022-07-28 06:45:27,061: INFO: Epoch: 235 Early stopping counter 32 of 50
2022-07-28 06:45:27,130: INFO: Epoch: 236 Early stopping counter 33 of 50
2022-07-28 06:45:27,195: INFO: Epoch: 237 Early stopping counter 34 of 50
2022-07-28 06:45:27,261: INFO: Epoch: 238 Early stopping counter 35 of 50
2022-07-28 06:45:27,327: INFO: Epoch: 239 Early stopping counter 36 of 50
2022-07-28 06:45:27,395: INFO: Epoch: 240 Early stopping counter 37 of 50
2022-07-28 06:45:27,465: INFO: Epoch: 241 Early stopping counter 38 of 50
2022-07-28 06:45:27,531: INFO: Epoch: 242 Early stopping counter 39 of 50
2022-07-28 06:45:27,601: INFO: Epoch: 243 Early stopping counter 40 of 50
2022-07-28 06:45:27,668: INFO: Epoch: 244 Early stopping counter 41 of 50
2022-07-28 06:45:27,739: INFO: Epoch: 245 Early stopping counter 42 of 50
2022-07-28 06:45:27,803: INFO: Epoch: 246 Early stopping counter 43 of 50
2022-07-28 06:45:27,872: INFO: Epoch: 247 Early stopping counter 44 of 50
2022-07-28 06:45:27,943: INFO: Epoch: 248 Early stopping counter 45 of 50
2022-07-28 06:45:28,009: INFO: Epoch: 249 Early stopping counter 46 of 50
2022-07-28 06:45:28,070: INFO: Epoch: 250 Early stopping counter 47 of 50
2022-07-28 06:45:28,131: INFO: Epoch: 251 Early stopping counter 48 of 50
2022-07-28 06:45:29,928: INFO: Epoch[ 251] Loss: 0.25	Accuracy: 93.333	Val_Loss: 2.603	Val_Acc: 30.000
2022-07-28 06:45:29,928: INFO: Epoch[ 251] Test Accuracy: 21.570
2022-07-28 06:45:29,962: INFO: Epoch: 252 Early stopping counter 49 of 50
2022-07-28 06:45:30,034: INFO: Epoch: 253 Early stopping counter 50 of 50
2022-07-28 06:45:30,034: INFO: Early stopping
2022-07-28 06:45:30,064: INFO: Trained for 253 Epochs.
2022-07-28 06:45:30,317: INFO: ('Accuracy on Train Set', 100.0)
2022-07-28 06:45:32,082: INFO: (2166, 'correctly labeled out of', 10000)
2022-07-28 06:45:32,082: INFO: ('Accuracy on Test Set:', 21.66)
2022-07-28 06:45:32,103: INFO: Saved model at cifar10/n100/with_train/Greedy_Model_100n_Epochs_1000_Early_Stop_253_Test_Acc_21.pth
2022-07-28 06:45:32,103: INFO: Training Complete
