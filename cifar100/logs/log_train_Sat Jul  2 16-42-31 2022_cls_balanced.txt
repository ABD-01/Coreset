2022-07-02 16:42:31,361: INFO: Hyperparameters
{'backbone': 'alexnet',
 'batch_size': 5000,
 'class_balanced': True,
 'config': 'src/configs/cifar100.yml',
 'criterion': 'NLLLoss',
 'criterion_kwargs': None,
 'dataset': 'cifar100',
 'dataset_dir': './data',
 'dont_train': False,
 'early_stopping_kwargs': {'min_delta': 0.001, 'patience': 12},
 'epochs': 1000,
 'logdir': PosixPath('cifar100/logs'),
 'lr': 0.01,
 'num_classes': 100,
 'num_workers': 2,
 'optimizer': 'sgd',
 'optimizer_kwargs': {'momentum': 0.9, 'weight_decay': 0.01},
 'output_dir': PosixPath('cifar100'),
 'per_class': False,
 'random': False,
 'resume': None,
 'scheduler': 'reduceonplateau',
 'scheduler_kwargs': {'factor': 0.2,
                      'min_lr': 1e-07,
                      'patience': 10,
                      'threshold': 0.001},
 'seed': 0,
 'test_model': None,
 'topn': 5000,
 'transformation_kwargs': {'normalize': {'mean': [0.5071, 0.4867, 0.4408],
                                         'std': [0.2675, 0.2565, 0.2761]}},
 'use_saved_best_inds': None,
 'val_percent': 0.1,
 'wandb': False}
2022-07-02 16:42:32,334: INFO: Dataset
Dataset CIFAR100
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               ToTensor()
               Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])
           )
2022-07-02 16:42:33,124: INFO: Test Dataset
Dataset CIFAR100
    Number of datapoints: 10000
    Root location: ./data
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])
           )
2022-07-02 16:42:33,146: INFO: all_similarities.shape: (100, 50000), all_imginds.shape: (100, 50000)
2022-07-02 16:42:38,976: INFO: Model Summary
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
├─Sequential: 1-1                        [-1, 1024]                --
|    └─Conv2d: 2-1                       [-1, 64, 16, 16]          1,792
|    └─MaxPool2d: 2-2                    [-1, 64, 8, 8]            --
|    └─ReLU: 2-3                         [-1, 64, 8, 8]            --
|    └─Conv2d: 2-4                       [-1, 192, 8, 8]           110,784
|    └─MaxPool2d: 2-5                    [-1, 192, 4, 4]           --
|    └─ReLU: 2-6                         [-1, 192, 4, 4]           --
|    └─Conv2d: 2-7                       [-1, 384, 4, 4]           663,936
|    └─ReLU: 2-8                         [-1, 384, 4, 4]           --
|    └─Conv2d: 2-9                       [-1, 256, 4, 4]           884,992
|    └─ReLU: 2-10                        [-1, 256, 4, 4]           --
|    └─Conv2d: 2-11                      [-1, 256, 4, 4]           590,080
|    └─MaxPool2d: 2-12                   [-1, 256, 2, 2]           --
|    └─ReLU: 2-13                        [-1, 256, 2, 2]           --
|    └─Flatten: 2-14                     [-1, 1024]                --
├─Sequential: 1-2                        [-1, 100]                 --
|    └─Dropout: 2-15                     [-1, 1024]                --
|    └─Linear: 2-16                      [-1, 512]                 524,800
|    └─ReLU: 2-17                        [-1, 512]                 --
|    └─Dropout: 2-18                     [-1, 512]                 --
|    └─Linear: 2-19                      [-1, 256]                 131,328
|    └─ReLU: 2-20                        [-1, 256]                 --
|    └─Linear: 2-21                      [-1, 100]                 25,700
|    └─LogSoftmax: 2-22                  [-1, 100]                 --
==========================================================================================
Total params: 2,933,412
Trainable params: 2,933,412
Non-trainable params: 0
Total mult-adds (M): 45.34
==========================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 0.33
Params size (MB): 11.19
Estimated Total Size (MB): 11.54
==========================================================================================
2022-07-02 16:42:47,665: INFO: Epoch[   1] Loss: 16.81	Accuracy: 1.133	Val_Loss: 5.938	Val_Acc: 0.800
2022-07-02 16:42:47,665: INFO: Epoch[   1] Test Accuracy: 0.700
2022-07-02 16:43:36,338: INFO: Early stopping counter 1 of 12
2022-07-02 16:43:38,673: INFO: Early stopping counter 1 of 12
2022-07-02 16:43:41,029: INFO: Early stopping counter 1 of 12
2022-07-02 16:43:43,409: INFO: Early stopping counter 1 of 12
2022-07-02 16:43:48,791: INFO: Epoch[  51] Loss: 4.62	Accuracy: 1.511	Val_Loss: 4.584	Val_Acc: 1.200
2022-07-02 16:43:48,792: INFO: Epoch[  51] Test Accuracy: 1.110
2022-07-02 16:44:49,947: INFO: Epoch[ 101] Loss: 4.54	Accuracy: 2.600	Val_Loss: 4.489	Val_Acc: 2.800
2022-07-02 16:44:49,947: INFO: Epoch[ 101] Test Accuracy: 3.320
2022-07-02 16:45:50,277: INFO: Epoch[ 151] Loss: 4.42	Accuracy: 3.733	Val_Loss: 4.359	Val_Acc: 5.000
2022-07-02 16:45:50,278: INFO: Epoch[ 151] Test Accuracy: 4.600
2022-07-02 16:46:50,934: INFO: Epoch[ 201] Loss: 4.29	Accuracy: 4.867	Val_Loss: 4.251	Val_Acc: 8.800
2022-07-02 16:46:50,934: INFO: Epoch[ 201] Test Accuracy: 6.740
2022-07-02 16:46:52,125: INFO: Early stopping counter 1 of 12
2022-07-02 16:47:07,456: INFO: Early stopping counter 1 of 12
2022-07-02 16:47:12,126: INFO: Early stopping counter 1 of 12
2022-07-02 16:47:16,781: INFO: Early stopping counter 1 of 12
2022-07-02 16:47:25,097: INFO: Early stopping counter 1 of 12
2022-07-02 16:47:29,789: INFO: Early stopping counter 1 of 12
2022-07-02 16:47:34,438: INFO: Early stopping counter 1 of 12
2022-07-02 16:47:36,791: INFO: Early stopping counter 1 of 12
2022-07-02 16:47:42,686: INFO: Early stopping counter 1 of 12
2022-07-02 16:47:49,761: INFO: Early stopping counter 1 of 12
2022-07-02 16:47:51,549: INFO: Epoch[ 251] Loss: 4.17	Accuracy: 6.178	Val_Loss: 4.149	Val_Acc: 9.200
2022-07-02 16:47:51,549: INFO: Epoch[ 251] Test Accuracy: 7.440
2022-07-02 16:47:55,047: INFO: Early stopping counter 1 of 12
2022-07-02 16:47:58,653: INFO: Early stopping counter 1 of 12
2022-07-02 16:48:02,169: INFO: Early stopping counter 1 of 12
2022-07-02 16:48:05,636: INFO: Early stopping counter 1 of 12
2022-07-02 16:48:07,952: INFO: Early stopping counter 1 of 12
2022-07-02 16:48:11,471: INFO: Early stopping counter 1 of 12
2022-07-02 16:48:13,869: INFO: Early stopping counter 1 of 12
2022-07-02 16:48:15,003: INFO: Early stopping counter 2 of 12
2022-07-02 16:48:18,513: INFO: Early stopping counter 1 of 12
2022-07-02 16:48:20,852: INFO: Early stopping counter 1 of 12
2022-07-02 16:48:22,032: INFO: Early stopping counter 2 of 12
2022-07-02 16:48:24,371: INFO: Early stopping counter 1 of 12
2022-07-02 16:48:27,898: INFO: Early stopping counter 1 of 12
2022-07-02 16:48:29,074: INFO: Early stopping counter 2 of 12
2022-07-02 16:48:31,382: INFO: Early stopping counter 1 of 12
2022-07-02 16:48:34,874: INFO: Early stopping counter 1 of 12
2022-07-02 16:48:37,199: INFO: Early stopping counter 1 of 12
2022-07-02 16:48:39,517: INFO: Early stopping counter 1 of 12
2022-07-02 16:48:41,865: INFO: Early stopping counter 1 of 12
2022-07-02 16:48:43,027: INFO: Early stopping counter 2 of 12
2022-07-02 16:48:44,180: INFO: Early stopping counter 3 of 12
2022-07-02 16:48:46,545: INFO: Early stopping counter 1 of 12
2022-07-02 16:48:47,730: INFO: Early stopping counter 2 of 12
2022-07-02 16:48:48,887: INFO: Early stopping counter 3 of 12
2022-07-02 16:48:50,077: INFO: Early stopping counter 4 of 12
2022-07-02 16:48:51,797: INFO: Epoch[ 301] Loss: 4.07	Accuracy: 7.689	Val_Loss: 4.076	Val_Acc: 8.200
2022-07-02 16:48:51,797: INFO: Epoch[ 301] Test Accuracy: 7.610
2022-07-02 16:48:55,281: INFO: Early stopping counter 1 of 12
2022-07-02 16:48:56,491: INFO: Early stopping counter 2 of 12
2022-07-02 16:48:57,755: INFO: Early stopping counter 3 of 12
2022-07-02 16:49:03,600: INFO: Early stopping counter 1 of 12
2022-07-02 16:49:04,754: INFO: Early stopping counter 2 of 12
2022-07-02 16:49:05,949: INFO: Early stopping counter 3 of 12
2022-07-02 16:49:07,120: INFO: Early stopping counter 4 of 12
2022-07-02 16:49:08,307: INFO: Early stopping counter 5 of 12
2022-07-02 16:49:09,474: INFO: Early stopping counter 6 of 12
2022-07-02 16:49:10,728: INFO: Early stopping counter 7 of 12
2022-07-02 16:49:13,170: INFO: Early stopping counter 1 of 12
2022-07-02 16:49:14,361: INFO: Early stopping counter 2 of 12
2022-07-02 16:49:15,559: INFO: Early stopping counter 3 of 12
2022-07-02 16:49:16,716: INFO: Early stopping counter 4 of 12
2022-07-02 16:49:17,902: INFO: Early stopping counter 5 of 12
2022-07-02 16:49:19,066: INFO: Early stopping counter 6 of 12
2022-07-02 16:49:20,243: INFO: Early stopping counter 7 of 12
2022-07-02 16:49:22,624: INFO: Early stopping counter 1 of 12
2022-07-02 16:49:26,115: INFO: Early stopping counter 1 of 12
2022-07-02 16:49:28,488: INFO: Early stopping counter 1 of 12
2022-07-02 16:49:29,654: INFO: Early stopping counter 2 of 12
2022-07-02 16:49:31,977: INFO: Early stopping counter 1 of 12
2022-07-02 16:49:33,149: INFO: Early stopping counter 2 of 12
2022-07-02 16:49:37,831: INFO: Early stopping counter 1 of 12
2022-07-02 16:49:40,218: INFO: Early stopping counter 1 of 12
2022-07-02 16:49:43,776: INFO: Early stopping counter 1 of 12
2022-07-02 16:49:46,140: INFO: Early stopping counter 1 of 12
2022-07-02 16:49:47,311: INFO: Early stopping counter 2 of 12
2022-07-02 16:49:50,796: INFO: Early stopping counter 1 of 12
2022-07-02 16:49:52,564: INFO: Epoch[ 351] Loss: 3.95	Accuracy: 8.178	Val_Loss: 3.960	Val_Acc: 10.400
2022-07-02 16:49:52,564: INFO: Epoch[ 351] Test Accuracy: 9.160
2022-07-02 16:49:54,884: INFO: Early stopping counter 1 of 12
2022-07-02 16:49:56,078: INFO: Early stopping counter 2 of 12
2022-07-02 16:49:58,463: INFO: Early stopping counter 1 of 12
2022-07-02 16:50:02,076: INFO: Early stopping counter 1 of 12
2022-07-02 16:50:04,448: INFO: Early stopping counter 1 of 12
2022-07-02 16:50:08,189: INFO: Early stopping counter 1 of 12
2022-07-02 16:50:09,401: INFO: Early stopping counter 2 of 12
2022-07-02 16:50:10,569: INFO: Early stopping counter 3 of 12
2022-07-02 16:50:11,753: INFO: Early stopping counter 4 of 12
2022-07-02 16:50:12,956: INFO: Early stopping counter 5 of 12
2022-07-02 16:50:16,501: INFO: Early stopping counter 1 of 12
2022-07-02 16:50:17,674: INFO: Early stopping counter 2 of 12
2022-07-02 16:50:18,851: INFO: Early stopping counter 3 of 12
2022-07-02 16:50:20,100: INFO: Early stopping counter 4 of 12
2022-07-02 16:50:21,275: INFO: Early stopping counter 5 of 12
2022-07-02 16:50:23,633: INFO: Early stopping counter 1 of 12
2022-07-02 16:50:24,796: INFO: Early stopping counter 2 of 12
2022-07-02 16:50:25,960: INFO: Early stopping counter 3 of 12
2022-07-02 16:50:27,155: INFO: Early stopping counter 4 of 12
2022-07-02 16:50:28,386: INFO: Early stopping counter 5 of 12
2022-07-02 16:50:29,584: INFO: Early stopping counter 6 of 12
2022-07-02 16:50:30,817: INFO: Early stopping counter 7 of 12
2022-07-02 16:50:32,067: INFO: Early stopping counter 8 of 12
2022-07-02 16:50:33,324: INFO: Early stopping counter 9 of 12
2022-07-02 16:50:34,501: INFO: Early stopping counter 10 of 12
2022-07-02 16:50:35,694: INFO: Early stopping counter 11 of 12
2022-07-02 16:50:36,887: INFO: Early stopping counter 12 of 12
2022-07-02 16:50:36,887: INFO: Early stopping
2022-07-02 16:50:36,930: INFO: Trained for 388 Epochs.
2022-07-02 16:50:37,223: DEBUG: [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.002, 0.002]
2022-07-02 16:50:38,239: INFO: ('Accuracy on Train Set', 11.911111325025558)
2022-07-02 16:50:40,423: INFO: (991, 'correctly labeled out of', 10000)
2022-07-02 16:50:40,423: INFO: ('Accuracy on Test Set:', 9.91)
2022-07-02 16:50:40,452: INFO: Saved model at cifar100/Greedy_Model_5000n_Epochs_1000_Early_Stop_388_Test_Acc_9__clsbalanced.pth
2022-07-02 16:50:40,452: INFO: Training Complete
